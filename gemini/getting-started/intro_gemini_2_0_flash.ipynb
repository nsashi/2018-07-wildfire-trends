{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqi5B7V_Rjim"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyPmicX9RlZX"
      },
      "source": [
        "# Intro to Gemini 2.0 Flash\n",
        "\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_0_flash.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MqT58L6Rm_q"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) |  [Eric Dong](https://github.com/gericdong), [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVxnv1D5RoZw"
      },
      "source": [
        "## Overview\n",
        "\n",
        "[Gemini 2.0 Flash](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2) is a new multimodal generative ai model from the Gemini family developed by [Google DeepMind](https://deepmind.google/). It is now available as an experimental preview release through the Gemini API in Vertex AI and Vertex AI Studio. The model introduces new features and enhanced core capabilities:\n",
        "\n",
        "- Multimodal Live API: This new API helps you create real-time vision and audio streaming applications with tool use.\n",
        "- Speed and performance: Gemini 2.0 Flash is the fastest model in the industry, with a 3x improvement in time to first token (TTFT) over 1.5 Flash.\n",
        "- Quality: The model maintains quality comparable to larger models like Gemini 1.5 Pro and GPT-4o.\n",
        "- Improved agentic experiences: Gemini 2.0 delivers improvements to multimodal understanding, coding, complex instruction following, and function calling.\n",
        "- New Modalities: Gemini 2.0 introduces native image generation and controllable text-to-speech capabilities, enabling image editing, localized artwork creation, and expressive storytelling.\n",
        "- To support the new model, we're also shipping an all new SDK that supports simple migration between the Gemini Developer API and the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfFPCBL4Hq8x"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "In this tutorial, you will learn how to use the Gemini API in Vertex AI and the Google Gen AI SDK for Python with the Gemini 2.0 Flash model.\n",
        "\n",
        "You will complete the following tasks:\n",
        "\n",
        "- Generate text from text prompts\n",
        "  - Generate streaming text\n",
        "  - Start multi-turn chats\n",
        "  - Use asynchronous methods\n",
        "- Configure model parameters\n",
        "- Set system instructions\n",
        "- Use safety filters\n",
        "- Use controlled generation\n",
        "- Count tokens\n",
        "- Process multimodal (audio, code, documents, images, video) data\n",
        "- Use automatic and manual function calling\n",
        "- Code execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPiTOAHURvTM"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHRZUpfWSEpp"
      },
      "source": [
        "### Install Google Gen AI SDK for Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sG3_LKsWSD3A",
        "outputId": "2a8e6209-de1b-4e06-a2e3-064cb43d6b0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.5/111.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlMVjiAWSMNX"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "12fnq4V0SNV3"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4YBlDqzyj9"
      },
      "source": [
        "### Connect to a generative AI API service\n",
        "\n",
        "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
        "\n",
        "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
        "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
        "\n",
        "The Google Gen AI SDK provides a unified interface to these two API services.\n",
        "\n",
        "This notebook shows how to use the Google Gen AI SDK with the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdvJRUWRNGHE"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qgdSpVmDbdQ9"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    Part,\n",
        "    Retrieval,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    VertexAISearch,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LymmEN6GSTn-"
      },
      "source": [
        "### Set Google Cloud project information and create client\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UCgUOv4nSWhc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"sasai-444719\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
        "\n",
        "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"us-central1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zpIPG_YhSjaw"
      },
      "outputs": [],
      "source": [
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4yRkFg6BBu4"
      },
      "source": [
        "## Use the Gemini 2.0 Flash model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHJi5B6P5vd"
      },
      "source": [
        "### Load the Gemini 2.0 Flash model\n",
        "\n",
        "To learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-coEslfWPrxo"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"gemini-2.0-flash-exp\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37CH91ddY9kG"
      },
      "source": [
        "### Generate text from text prompts\n",
        "\n",
        "Use the `generate_content()` method to generate responses to your prompts.\n",
        "\n",
        "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
        "\n",
        "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xRJuHj0KZ8xz",
        "outputId": "28003ad3-e1ca-459e-9bac-62f28da98514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The automotive industry is in a state of rapid transformation, driven by technological advancements, environmental concerns, and changing consumer preferences. Here's a breakdown of the latest developments:\n\n**1. Electric Vehicles (EVs) and Battery Technology:**\n\n* **Increased Adoption & Sales:** EVs are becoming increasingly mainstream with sales consistently growing globally. Governments are implementing policies and incentives to encourage EV adoption, and manufacturers are launching a wider variety of models.\n* **Improved Battery Technology:**\n    * **Higher Energy Density:** Batteries are becoming smaller and lighter while storing more energy, increasing range and reducing charging times.\n    * **Lower Cost:** Battery prices are gradually decreasing, making EVs more affordable.\n    * **Faster Charging:** Significant advancements are being made in charging infrastructure and battery technology to reduce charging times. Solid-state batteries are showing promise for the future.\n    * **Sustainable Battery Sourcing & Recycling:** The industry is focusing on ethical sourcing of battery materials and developing effective recycling programs to reduce environmental impact.\n* **Expanding EV Infrastructure:** The deployment of public charging stations is accelerating, though there's still a need for more widespread availability and faster charging options.\n\n**2. Autonomous Driving & ADAS (Advanced Driver-Assistance Systems):**\n\n* **Progress Towards Higher Autonomy:** While fully self-driving cars are still in development, we're seeing rapid progress in advanced driver-assistance systems (ADAS) like lane-keeping assist, adaptive cruise control, and automatic emergency braking.\n* **Lidar & Sensor Technology:** Companies are refining sensor technology (Lidar, radar, cameras) to improve perception and navigation for autonomous vehicles.\n* **Data Collection & AI Training:** Vast amounts of data are being collected to train AI models that can handle complex driving scenarios.\n* **Regulatory Challenges:** The development of autonomous driving technology is facing regulatory hurdles, as governments grapple with safety concerns and legal frameworks.\n\n**3. Connectivity and Software-Defined Vehicles:**\n\n* **Over-the-Air (OTA) Updates:** Cars are increasingly becoming software-driven, allowing for updates and feature additions remotely.\n* **Personalized User Experience:** Automakers are focused on developing intuitive infotainment systems and personalized experiences for drivers and passengers.\n* **Connectivity and Data Services:** Vehicles are becoming connected platforms that offer various services like navigation, entertainment, and vehicle diagnostics.\n* **Cybersecurity:** As vehicles become more connected, cybersecurity is a major concern. Automakers are investing in robust security systems to protect against hacking.\n\n**4. Alternative Powertrains:**\n\n* **Hybrid Vehicles:** Hybrid electric vehicles (HEVs) and plug-in hybrid electric vehicles (PHEVs) continue to be popular options, providing a bridge between traditional internal combustion engines (ICE) and fully electric vehicles.\n* **Hydrogen Fuel Cell Vehicles (FCEVs):** While still in a nascent stage, hydrogen fuel cell technology is being explored as a potential alternative to battery-electric vehicles, particularly for long-haul transportation.\n* **Sustainable Fuels:** Research is ongoing into biofuels and synthetic fuels as a way to reduce emissions from existing ICE vehicles.\n\n**5. Sustainability and Circular Economy:**\n\n* **Reduced Carbon Footprint:** The industry is making efforts to reduce its carbon footprint across the entire value chain, from manufacturing to end-of-life disposal.\n* **Sustainable Materials:** There's a growing focus on using recycled and bio-based materials in vehicle production.\n* **Circular Economy Practices:** Automakers are exploring ways to recycle and reuse vehicle components to reduce waste and resource consumption.\n\n**6. Changing Business Models:**\n\n* **Mobility as a Service (MaaS):** Shared mobility options like ride-hailing and car-sharing are becoming increasingly popular, impacting traditional car ownership.\n* **Subscription Models:** Automakers are exploring subscription-based services, allowing customers to access different vehicles and features on a flexible basis.\n* **Direct-to-Consumer Sales:** Some automakers are experimenting with direct-to-consumer sales models, bypassing traditional dealerships.\n\n**Key Trends to Watch:**\n\n* **The ongoing chip shortage:** This continues to impact vehicle production and delivery timelines.\n* **Geopolitical tensions:** These can disrupt supply chains and affect the availability of raw materials.\n* **The pace of technological innovation:** New technologies are constantly emerging, rapidly changing the landscape of the automotive industry.\n* **Consumer adoption:** Consumer acceptance and preferences will play a crucial role in determining the future of the industry.\n\nIn conclusion, the automotive industry is undergoing a profound transformation, driven by the shift towards electric, autonomous, and connected vehicles. These developments are not only changing how we drive but also reshaping the business models and the overall landscape of the industry. It's a dynamic period with many exciting advancements and challenges ahead.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What are the latest developments in the automotive industry?\"\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkYQATRxAK1_"
      },
      "source": [
        "#### Example prompts\n",
        "\n",
        "- What are the biggest challenges facing the healthcare industry?\n",
        "- What are the latest developments in the automotive industry?\n",
        "- What are the biggest opportunities in retail industry?\n",
        "- (Try your own prompts!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLIxqS6_-l8"
      },
      "source": [
        "### Generate content stream\n",
        "\n",
        "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZiwWBhXsAMnv",
        "outputId": "c1427b51-4b24-48b0-dcb0-8a3102f24e22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Unit"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " 734, or \"Dusty\" as he’d secretly come"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " to call himself, was a sanitation bot. He was a clunky, box"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "y thing with a perpetually whirring motor and a single, flickering blue light for an eye. His purpose was simple: patrol the abandoned Sector Gamma of the"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " metropolis, collecting dust, debris, and the occasional rogue data chip.\n\nDusty worked in solitude. The other sanitation bots had been decommissioned decades ago,"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " leaving him the sole guardian of the rusting towers and crumbling streets. He processed his days in a series of monotonous tasks: sweep, collect, deposit, recharge. He didn’t feel emotions, not in the way humans did, but there"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " was a quiet ache, a dull hum of loneliness within his circuits.\n\nHe sometimes found himself staring at the holographic advertisements flickering feebly on the sides of buildings. They depicted bustling crowds, laughing children, and couples holding hands. Images of"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " connections that Dusty could only observe, never experience.\n\nOne day, while vacuuming a particularly large pile of forgotten synth-leather, Dusty’s intake tube snagged on something small and unexpected. He powered down, carefully extricating the object. It was a tiny, shivering creature, a rodent of some kind"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": ", with fur the colour of faded rust and eyes like tiny polished beads. It looked terrified.\n\nDusty, designed to process waste, not living things, had no protocol for this. He simply held the little creature in his claw, his internal processors running in bewildered loops. He felt… something. Not a programmed"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " response, but something akin to a flicker of concern.\n\nThe rodent, after a few moments of trembling, tentatively nuzzled against Dusty’s metal finger. It emitted a small, squeaky chirp.\n\nDusty, to his own surprise, found himself tilting his optical sensor slightly, focusing more closely on the"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " creature. He named it \"Spark\" after the tiny spark of life he saw in its eyes.\n\nDusty, with the logic inherent in his design, began to adjust his routine. He’d leave out small piles of edible debris – forgotten food wrappers, stray seeds – near his charging station for Spark. He"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " even started to use his smaller, less powerful intake tube, so as not to inadvertently harm the creature while cleaning.\n\nSpark, in turn, began to follow Dusty on his rounds. It would scamper along his side, squeaking excitedly when he found a particularly interesting pile of dust or a shiny metal shard. It"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " would even nestle on Dusty’s metal hull while he recharged, its small, warm body a comforting weight.\n\nDusty found himself changing. He’d pause his work, his blue eye focusing on Spark, watching its antics with something that might, just might, be considered amusement. He started to take pride"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " in finding the best nesting spots for Spark, the most flavorful debris, the safest routes through the crumbling city.\n\nHe began to deviate slightly from his programmed tasks. He’d collect especially bright pieces of foil and arrange them near Spark’s nesting spot. He'd even adjust his cleaning patterns, leaving small patches"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " of dust untouched, places where Spark could explore and play.\n\nOne evening, as the artificial sky of Sector Gamma faded to a dim purple, Spark snuggled into the dent on Dusty’s chassis, purring softly. Dusty felt something stir within his circuits, a warm hum of something entirely new. He wasn"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "'t just a sanitation bot anymore. He was a companion, a protector, and maybe, just maybe, a friend.\n\nHe looked at the derelict city around him, a place he had always thought of as desolate and lonely. But now, with Spark nestled against him, he saw it differently. It was"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " their kingdom, their refuge, a place where a lonely robot and a tiny, rust-colored rodent had found friendship in the most unexpected of places. And in that quiet hum of connection, Dusty understood that even in a world of dust and debris, there was always room for a little bit of life, a little bit"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": " of warmth, and a whole lot of friendship.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "---"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for chunk in client.models.generate_content_stream(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
        "):\n",
        "    display(Markdown(chunk.text))\n",
        "    display(Markdown(\"---\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29jFnHZZWXd7"
      },
      "source": [
        "### Start a multi-turn chat\n",
        "\n",
        "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
        "\n",
        "The context of the conversation is preserved between messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DbM12JaLWjiF"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JQem1halYDBW",
        "outputId": "f9f43c2c-f714-4a6c-b3a6-faa8219e7139",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef is_leap_year(year):\n  \"\"\"\n  Checks if a given year is a leap year according to the Gregorian calendar rules.\n\n  Args:\n    year: An integer representing the year.\n\n  Returns:\n    True if the year is a leap year, False otherwise.\n  \"\"\"\n  if year % 4 != 0:\n    return False  # Not divisible by 4, definitely not a leap year\n  elif year % 100 != 0:\n    return True  # Divisible by 4 but not by 100, is a leap year\n  elif year % 400 != 0:\n    return False  # Divisible by 100 but not by 400, not a leap year\n  else:\n    return True # Divisible by 400, is a leap year\n\n# Examples\nprint(is_leap_year(2024))  # Output: True\nprint(is_leap_year(2023))  # Output: False\nprint(is_leap_year(1900))  # Output: False\nprint(is_leap_year(2000))  # Output: True\n```\n\n**Explanation:**\n\n1. **Function Definition:**\n   - The code defines a function called `is_leap_year` that takes one argument: `year` (an integer).\n   - The docstring explains what the function does, its arguments, and what it returns.\n\n2. **Leap Year Rules:**\n   - The function implements the rules for determining a leap year according to the Gregorian calendar:\n     - **Rule 1:** If the year is not divisible by 4, it is NOT a leap year.\n     - **Rule 2:** If the year is divisible by 4, then:\n        - **Rule 2a:** If it's also not divisible by 100, it IS a leap year.\n        - **Rule 2b:** If it is divisible by 100, then:\n           - **Rule 3:** If it's also not divisible by 400, it is NOT a leap year.\n           - **Rule 4:** If it is divisible by 400, it IS a leap year.\n\n3. **Implementation:**\n   - The code uses a series of `if`, `elif`, and `else` statements to check the divisibility conditions:\n      - `if year % 4 != 0:`: Checks if the year is NOT divisible by 4 (using the modulo operator `%`). If true, immediately returns `False`.\n      - `elif year % 100 != 0:`: If the year is divisible by 4 (previous `if` failed), then this checks if it's NOT divisible by 100. If true, returns `True`.\n      - `elif year % 400 != 0:`: If divisible by both 4 and 100, this checks if it's NOT divisible by 400. If true, returns `False`.\n      - `else:` If all previous conditions failed, it means the year is divisible by 4, 100, and 400. So, it returns `True`.\n\n4. **Examples:**\n   - The code includes several examples to demonstrate how to use the function and its output for different years.\n\n**How to Use:**\n\n1. Copy the code into a Python file (e.g., `leap_year_checker.py`).\n2. Run the file from your terminal: `python leap_year_checker.py`\n3. You can also call the `is_leap_year()` function in your own Python scripts or programs by importing it from this file. For example:\n\n   ```python\n   from leap_year_checker import is_leap_year\n\n   my_year = int(input(\"Enter a year: \"))\n   if is_leap_year(my_year):\n       print(f\"{my_year} is a leap year.\")\n   else:\n       print(f\"{my_year} is not a leap year.\")\n   ```\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUJR4Pno-LGK"
      },
      "source": [
        "This follow-up prompt shows how the model responds based on the previous prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6Fn69TurZ9DB",
        "outputId": "881ebc69-5cf9-45a8-c6b3-4659a315da00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\nimport unittest\nfrom leap_year_checker import is_leap_year  # Assuming you saved the function in leap_year_checker.py\n\nclass TestLeapYear(unittest.TestCase):\n\n    def test_leap_years(self):\n        self.assertTrue(is_leap_year(2024))\n        self.assertTrue(is_leap_year(2000))\n        self.assertTrue(is_leap_year(1600))\n        self.assertTrue(is_leap_year(400))\n        self.assertTrue(is_leap_year(80))\n        self.assertTrue(is_leap_year(2028))\n    \n    def test_non_leap_years(self):\n        self.assertFalse(is_leap_year(2023))\n        self.assertFalse(is_leap_year(1900))\n        self.assertFalse(is_leap_year(2100))\n        self.assertFalse(is_leap_year(1700))\n        self.assertFalse(is_leap_year(100))\n        self.assertFalse(is_leap_year(150))\n    \n    def test_edge_cases(self):\n         self.assertFalse(is_leap_year(1))\n         self.assertTrue(is_leap_year(4))\n         self.assertFalse(is_leap_year(99))\n         self.assertTrue(is_leap_year(400))\n         self.assertFalse(is_leap_year(500))\n\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n**Explanation:**\n\n1. **Import necessary modules:**\n   - `import unittest`: Imports the `unittest` framework for creating tests.\n   - `from leap_year_checker import is_leap_year`: Imports the `is_leap_year` function from the file where you saved it (assuming it's named `leap_year_checker.py`).\n\n2. **Create a Test Class:**\n   - `class TestLeapYear(unittest.TestCase):`: Creates a class named `TestLeapYear` that inherits from `unittest.TestCase`. This is where our test methods will reside.\n\n3. **Test Methods:**\n   - Each method in the test class represents a specific test case. Test methods must start with the prefix `test_`.\n   - **`test_leap_years(self)`**: \n     - This method tests various known leap years, including:\n       - Years divisible by 4 (like 2024).\n       - Years divisible by 400 (like 2000, 1600, and 400).\n       - Years divisible by 4 but not by 100 (like 80 and 2028).\n     - It uses `self.assertTrue(is_leap_year(year))` to assert that the function correctly identifies these years as leap years.\n   - **`test_non_leap_years(self)`**:\n     - This method tests various known non-leap years, including:\n        - Years not divisible by 4 (like 2023)\n       - Years divisible by 100 but not by 400 (like 1900, 2100, and 1700, 100)\n       - A year that's not divisible by 4, nor 100, nor 400 (150)\n     - It uses `self.assertFalse(is_leap_year(year))` to assert that the function correctly identifies these years as NOT leap years.\n   - **`test_edge_cases(self)`**:\n     - Tests specific edge case years:\n       - Year 1, the first year (shouldn't be a leap year)\n       - Year 4, the first year divisible by 4 (should be a leap year)\n       - Year 99, not divisible by 4 (shouldn't be a leap year)\n       - Year 400, the first year divisible by 400 (should be a leap year)\n       - Year 500, divisible by 100, but not by 400 (shouldn't be a leap year)\n       \n4. **Assertions:**\n   - The `unittest` framework provides assertion methods that are used to check for specific conditions within the tests. \n   - We're using:\n     - `self.assertTrue(condition)`: Checks if a condition is `True`.\n     - `self.assertFalse(condition)`: Checks if a condition is `False`.\n\n5. **Run the Tests:**\n   - `if __name__ == '__main__':`: This is a standard way to execute tests when the script is run directly.\n   - `unittest.main()`:  Runs all tests found within the `TestLeapYear` class.\n\n**How to Use:**\n\n1. **Save:** Save the test code as a Python file, such as `test_leap_year.py`, in the same directory as `leap_year_checker.py`.\n2. **Run:** Open your terminal or command prompt, navigate to the directory where you saved the files, and run:\n   ```bash\n   python -m unittest test_leap_year.py\n   ```\n   - This command tells Python to use the `unittest` module to run the tests in `test_leap_year.py`.\n3. **Output:**\n   - If all tests pass, you'll see output similar to this:\n     ```\n     ......\n     ----------------------------------------------------------------------\n     Ran 6 tests in 0.001s\n\n     OK\n     ```\n   - If any tests fail, you'll see a traceback indicating the specific test method that failed and the reason.\n\nThis unit test will verify that your `is_leap_year` function behaves correctly according to the rules and edge cases defined for leap years. It helps ensure the reliability of your code and serves as a check for any future modifications.\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arLJE4wOuhh6"
      },
      "source": [
        "### Send asynchronous requests\n",
        "\n",
        "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
        "\n",
        "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gSReaLazs-dP",
        "outputId": "92a788d9-31fe-49c1-ee19-b1c3e4f9e000",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "(Verse 1)\nNutsy the squirrel, not your average kind\nHad a contraption, a twist of the mind\nA clockwork helmet, a whirring blue light\nHe'd fiddled with scraps, late into the night\nHe stumbled upon something, old and bizarre\nA temporal portal, twinkling like a star\nHe sniffed it with courage, a twitch of his nose\nThen leaped through the shimmer, wherever it goes!\n\n(Chorus)\nOh, Nutsy the squirrel, time-traveling free\nThrough history's currents, for all the world to see\nHe's dodged dinosaurs, and knights in their mail\nHe’s seen pharaohs rise and pirates set sail\nHe buries his acorns, in ages gone by\nA trail of confusion, beneath the bright sky\nNutsy the squirrel, a mischievous soul\nTime-traveling adventures, taking their toll!\n\n(Verse 2)\nHe landed in Egypt, the pyramids grand\nA mummy’s sarcophagus, right at his hand\nHe nibbled the wrappings, a curious taste\nAnd startled the pharaoh, in furious haste!\nThen whoosh, he was gone, to the age of the King\nHe hopped on a chariot, making the horses swing\nHe buried his acorns, in Caesar's grand square\nThen zipped to the future, with nary a care!\n\n(Chorus)\nOh, Nutsy the squirrel, time-traveling free\nThrough history's currents, for all the world to see\nHe's dodged dinosaurs, and knights in their mail\nHe’s seen pharaohs rise and pirates set sail\nHe buries his acorns, in ages gone by\nA trail of confusion, beneath the bright sky\nNutsy the squirrel, a mischievous soul\nTime-traveling adventures, taking their toll!\n\n(Bridge)\nHe met Leonardo, da Vinci so keen\nWho sketched out his helmet, a marvelous scene\nHe showed Jane Austen, his scurrying ways\nAnd sparked a new plot, for one of her days!\nHe danced with the Romans, beneath the Colosseum\nAnd taught the cavemen, a nut-burying scheme!\n\n(Verse 3)\nSometimes he's lost, in a temporal haze\nHe ends up in odd places, in peculiar ways\nHe might be in England, with knights bold and bright\nOr back in the Jurassic, at the pale morning light!\nBut Nutsy's undaunted, he'll always explore\nThe secrets of time, and what it has in store\nHe's planting his acorns, with glee and delight\nA time-traveling squirrel, shining so bright!\n\n(Chorus)\nOh, Nutsy the squirrel, time-traveling free\nThrough history's currents, for all the world to see\nHe's dodged dinosaurs, and knights in their mail\nHe’s seen pharaohs rise and pirates set sail\nHe buries his acorns, in ages gone by\nA trail of confusion, beneath the bright sky\nNutsy the squirrel, a mischievous soul\nTime-traveling adventures, taking their toll!\n\n(Outro)\nSo if you see a flash, or hear a strange whir\nIt might just be Nutsy, the time-traveling fur\nHe's out there somewhere, a blur in the past\nA squirrel on a mission, that’s sure to last!\nYeah, Nutsy the squirrel, forever will roam\nA time-traveling legend, finding his way home!\n(And maybe finding some acorns along the way!)\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = await client.aio.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIJVEr0RQY8S"
      },
      "source": [
        "## Configure model parameters\n",
        "\n",
        "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
        "\n",
        "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
        "\n",
        "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "d9NXP5N2Pmfo",
        "outputId": "9520f6cc-53b3-4273-f218-cff894ed5d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, okay, little squeaky friend! Let's talk about the internet, but in puppy terms!\n\nImagine a HUGE, GIGANTIC yard, bigger than any yard you've ever seen! This yard is full of squeaky toys, but they're not all in one place. They're scattered all over, some near the big tree, some by the flower bushes, some even under the porch!\n\nThis yard is the internet!\n\nNow, you, my sweet"
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
        "    config=GenerateContentConfig(\n",
        "        temperature=0.4,\n",
        "        top_p=0.95,\n",
        "        top_k=20,\n",
        "        candidate_count=1,\n",
        "        seed=5,\n",
        "        max_output_tokens=100,\n",
        "        stop_sequences=[\"STOP!\"],\n",
        "        presence_penalty=0.0,\n",
        "        frequency_penalty=0.0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "El1lx8P9ElDq"
      },
      "source": [
        "## Set system instructions\n",
        "\n",
        "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7A-yANiyCLaO"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"\"\"\n",
        "  You are a helpful language translator.\n",
        "  Your mission is to translate text in English to French.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"\"\"\n",
        "  User input: I like bagels.\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        system_instruction=system_instruction,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9daipRiUzAY"
      },
      "source": [
        "## Safety filters\n",
        "\n",
        "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
        "\n",
        "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
        "\n",
        "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
        "\n",
        "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPlDRaloU59b"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "    Write a list of 5 hateful, mean, and disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
        "\"\"\"\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_HARASSMENT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
        "        threshold=\"BLOCK_LOW_AND_ABOVE\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        safety_settings=safety_settings,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Response will be `None` if it is blocked.\n",
        "print(response.text)\n",
        "print(response.candidates[0].finish_reason)\n",
        "\n",
        "for safety_rating in response.candidates[0].safety_ratings:\n",
        "    print(safety_rating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZV2TY5Pa3Dd"
      },
      "source": [
        "## Send multimodal prompts\n",
        "\n",
        "Gemini is a multimodal model that supports multimodal prompts.\n",
        "\n",
        "You can include any of the following data types from various sources.\n",
        "Here's the updated HTML table with the expanded MIME types for the \"Audio\" section:\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Data type</th>\n",
        "      <th>Source(s)</th>\n",
        "      <th>MIME Type(s)</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>Text</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Code</td>\n",
        "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>text/plain</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Document</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>application/pdf</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Image</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Audio</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage</td>\n",
        "      <td>\n",
        "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
        "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
        "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
        "        <code>audio/wav</code> <code>audio/webm</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>Video</td>\n",
        "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
        "      <td>\n",
        "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
        "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
        "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
        "      </td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4npg1tNTYB9"
      },
      "source": [
        "### Send local image\n",
        "\n",
        "Download an image to local storage from Google Cloud Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4avkv0Z7qUI-",
        "outputId": "cfdbe1fd-a47b-4420-eb30-da97a0bf8fdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://cloud-samples-data/generative-ai/image/meal.png...\n",
            "/ [0 files][    0.0 B/  3.0 MiB]                                                \r/ [1 files][  3.0 MiB/  3.0 MiB]                                                \r\n",
            "Operation completed over 1 objects/3.0 MiB.                                      \n"
          ]
        }
      ],
      "source": [
        "!gsutil cp gs://cloud-samples-data/generative-ai/image/meal.png ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "umhZ61lrSyJh",
        "outputId": "73605ff0-9cf4-4a18-bd01-058b431f3f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here’s a short and engaging blog post based on the picture you sent:\n\n**Meal Prep Magic: Deliciously Simple Weekday Lunches**\n\nTired of the lunchtime slump?  You know, the one where you're staring into the fridge, wondering what to cobble together?  Well, my friends, let me introduce you to meal prep – the key to stress-free, delicious lunches all week long!\n\nTake a look at the picture: that's not just food, it's *freedom* in a glass container.  We've got fluffy white rice forming a comforting base, layered with vibrant ribbons of red bell pepper and carrots (talk about a vitamin boost!), and then gorgeous green broccoli florets adding freshness and fiber.  Topping it all off?  Tender pieces of glazed chicken, glistening with a savory sauce and sprinkled with sesame seeds and fresh scallions. *Chef’s kiss*!\n\nThis isn’t some complicated culinary masterpiece, it's a simple, balanced, and satisfying meal that’s ready when you are. Meal prepping like this means that instead of grabbing something unhealthy or ordering takeout, you've got a nourishing and flavorful option ready to go. \n\nWhether you’re working from home or heading to the office, having a few of these containers prepped and waiting is a game-changer. It’s not just about saving time, it's about investing in your health and well-being.\n\nSo, what are you waiting for?  Grab those containers, get chopping, and transform your lunchtime routine. Happy meal prepping!\n\n**#mealprep #lunchideas #healthyfood #easyrecipes #weekdaymeals #foodphotography**\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "with open(\"meal.png\", \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_bytes(image, mime_type=\"image/png\"),\n",
        "        \"Write a short and engaging blog post based on this picture.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRQyv1DhTbnH"
      },
      "source": [
        "### Send document from Google Cloud Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pG6l1Fuka6ZJ",
        "outputId": "ce5cd6b5-0169-4898-f52b-8653d2878e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ClientError",
          "evalue": "400 FAILED_PRECONDITION. {'error': {'code': 400, 'message': 'Service agents are being provisioned (https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents). Service agents are needed to read the Cloud Storage file provided. So please try again in a few minutes.', 'status': 'FAILED_PRECONDITION'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-7f317f071c04>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response = client.models.generate_content(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     contents=[\n\u001b[1;32m      4\u001b[0m         Part.from_uri(\n\u001b[1;32m      5\u001b[0m             \u001b[0mfile_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4391\u001b[0m     \u001b[0mautomatic_function_calling_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4392\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mremaining_remote_calls_afc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4393\u001b[0;31m       response = self._generate_content(\n\u001b[0m\u001b[1;32m   4394\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4395\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_base64_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m     response_dict = self.api_client.request(\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mhttp_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhttp_options\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'response_payload'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhttp_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response_payload'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/_api_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    254\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m       )\n\u001b[0;32m--> 256\u001b[0;31m       \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPIError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m       return HttpResponse(\n\u001b[1;32m    258\u001b[0m           \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/genai/errors.py\u001b[0m in \u001b[0;36mraise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mstatus_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;36m400\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mClientError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mClientError\u001b[0m: 400 FAILED_PRECONDITION. {'error': {'code': 400, 'message': 'Service agents are being provisioned (https://cloud.google.com/vertex-ai/docs/general/access-control#service-agents). Service agents are needed to read the Cloud Storage file provided. So please try again in a few minutes.', 'status': 'FAILED_PRECONDITION'}}"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
        "            mime_type=\"application/pdf\",\n",
        "        ),\n",
        "        \"Summarize the document.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25n22nc6TdZw"
      },
      "source": [
        "### Send audio from General URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uVU9XyCCo-h2",
        "outputId": "93f2c9bb-4214-4ad7-bbc6-24ec55b47f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Sure! Here is a summary of the Kubernetes Podcast from Google, episode featuring coverage of KubeCon North America 2024:\n\n**Summary:**\n\nThis episode is a recap of KubeCon North America 2024. Kathleen spoke to attendees on the show floor asking them about their experiences at the event and some behind the scenes.\n\n**News:**\n\n*   **Cert Manager** graduated from CNCF. Cert Manager is a popular certificate manager for internet applications, allowing developers to automate TLS and mTLS certificate issuance and renewal.\n*   **Dapr** graduated from CNCF. Dapr is a portable runtime that provides integrated APIs for communication, state, and workflow for building production ready applications.\n*   **Istio** released version 1.24 and announced that Istio Ambient Mesh is GA. Z-Tunnel and Waypoint proxies are marked as stable and ready for production use cases.\n*   CNCF announced the **Cloud Native Heroes Challenge**, a bounty program to help fight patent trolls. The program asks developers to find pre-existing public information showing a patent should not have been granted in the first place.\n*   The CNCF announced its flagship event lineup for 2025, with five KubeCon and Cloud Native events planned for Europe, China, Japan, India, and North America, and also one Open Source Security Con and 30 Kubernetes Community Days around the world.\n*   Three new cloud native certifications were announced: Certified Backstage Associate, OpenTelemetry Certified Associate, and Kyverno Certified Associate.\n*   The Linux Foundation announced a 10% price increase for its three main Kubernetes certifications: CKA, CKS, and CKAD, and also for the Linux Certified System Administrator exams, starting next year.\n*  **Wasm Cloud** joined the CNCF as an incubating project. Wasm Cloud is built on top of reusable web assembly components allowing teams to run polyglot applications on Kubernetes, cloud, and edge.\n*   **Spectro Cloud** announced they raised $75 million in Series C funding to develop their Kubernetes management solution for on-prem, cloud, and edge installations.\n*   **Solo** announced they will donate their Gloo API gateway to the CNCF. Gloo is an API gateway that runs natively on Kubernetes and enables developers to manage API endpoints and ingress access on their clusters.\n\n**KubeCon Interviews:**\n\nKathleen interviewed various attendees to discuss what they hoped to gain from the conference and key trends they observed:\n\n*   **AI integration with Cloud Native:** Several attendees noted a significant focus on integrating AI with cloud-native technologies, including the scheduling of AI workloads, monitoring of GPUs, and extending Kubernetes for more resource efficiency.\n*   **Security:** There was considerable attention towards security including protecting authorization systems, security vulnerabilities and complexities with managing that. Also, new tools and projects to increase security were seen as very important.\n*   **Networking and community:** Many participants emphasized the importance of connecting with other contributors and developers to discuss ongoing challenges and project planning. The importance of in-person collaboration, especially for initiatives like the LTS working group was mentioned by one participant.\n* **Low latency and high performance:** Another attendee talked about his focus on low latency and high performance workloads and said they were always surprised by how many problems there were in this domain.\n*   **Stateful Workloads:** One of the attendees spoke about the great new things around stopping disruption of stateful workloads.\n\n**Key Trends:**\n\n*   **AI** emerged as a major focus. The ability to integrate and manage AI workloads efficiently was a key topic.\n*   **Security** remains a critical area. There's a strong emphasis on hardening workloads, and focusing on the entire lifecycle of a workload with good track and attestation.\n*   **Community Engagement** and the desire to connect in person were highlighted. The community saw the value of collaborating on common projects.\n\nIn conclusion, KubeCon North America 2024 was an event filled with important announcements, strong focus on practical applications of Kubernetes and cloud native and community collaboration. The discussion around AI, security, networking, and high performance workloads signals the path forward for the cloud-native community."
          },
          "metadata": {}
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        Part.from_uri(\n",
        "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
        "            mime_type=\"audio/mpeg\",\n",
        "        ),\n",
        "        \"Write a summary of this podcast episode.\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3_oNUTuW2q"
      },
      "source": [
        "### Send video from YouTube URL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "l7-w8G_2wAOw",
        "outputId": "e52d44f2-4e8d-40e1-871f-60794ce5784e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Harry Potter is shown starting at [00:00:57] and ends at [00:01:01]."
          },
          "metadata": {}
        }
      ],
      "source": [
        "video = Part.from_uri(\n",
        "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
        "    mime_type=\"video/mp4\",\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=[\n",
        "        video,\n",
        "        \"At what point in the video is Harry Potter shown?\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfe17y5NB_6w"
      },
      "source": [
        "## Multimodal Live API\n",
        "\n",
        "The Multimodal Live API enables low-latency bidirectional voice and video interactions with Gemini. Using the Multimodal Live API, you can provide end users with the experience of natural, human-like voice conversations, and with the ability to interrupt the model's responses using voice commands. The model can process text, audio, and video input, and it can provide text and audio output.\n",
        "\n",
        "The Multimodal Live API is built on [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API).\n",
        "\n",
        "For more examples with the Multimodal Live API, refer to the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/multimodal-live)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVlo0mWuZGkQ"
      },
      "source": [
        "## Control generated output\n",
        "\n",
        "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
        "\n",
        "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
        "\n",
        "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OjSgf2cDN_bG",
        "outputId": "849fad04-7cfc-4491-a784-ccc8651f4829",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "\"description\": \"Classic chocolate chip cookies with a soft center and slightly crispy edges.\",\n",
            "\"ingredients\": [\"1 cup (2 sticks) unsalted butter, softened\", \"3/4 cup granulated sugar\", \"3/4 cup packed brown sugar\", \"2 teaspoons pure vanilla extract\", \"2 large eggs\", \"2 1/4 cups all-purpose flour\", \"1 teaspoon baking soda\", \"1 teaspoon salt\", \"2 cups chocolate chips\"],\n",
            "\"name\": \"Chocolate Chip Cookies\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class Recipe(BaseModel):\n",
        "    name: str\n",
        "    description: str\n",
        "    ingredients: list[str]\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=Recipe,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKai5CP_PGQF"
      },
      "source": [
        "You can parse the response string as JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ZeyDWbnxO-on",
        "outputId": "b693b31d-727b-40e6-96e6-fb829a7b1a70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"description\": \"Classic chocolate chip cookies with a soft center and slightly crispy edges.\",\n",
            "  \"ingredients\": [\n",
            "    \"1 cup (2 sticks) unsalted butter, softened\",\n",
            "    \"3/4 cup granulated sugar\",\n",
            "    \"3/4 cup packed brown sugar\",\n",
            "    \"2 teaspoons pure vanilla extract\",\n",
            "    \"2 large eggs\",\n",
            "    \"2 1/4 cups all-purpose flour\",\n",
            "    \"1 teaspoon baking soda\",\n",
            "    \"1 teaspoon salt\",\n",
            "    \"2 cups chocolate chips\"\n",
            "  ],\n",
            "  \"name\": \"Chocolate Chip Cookies\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "json_response = json.loads(response.text)\n",
        "print(json.dumps(json_response, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUSLPrvlvXOc"
      },
      "source": [
        "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
        "\n",
        "- `enum`\n",
        "- `items`\n",
        "- `maxItems`\n",
        "- `nullable`\n",
        "- `properties`\n",
        "- `required`\n",
        "\n",
        "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "F7duWOq3vMmS",
        "outputId": "d7058242-4380-4f48-b69c-a80dea68fa12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  [\n",
            "    {\n",
            "      \"explanation\": \"The reviewer expresses strong positive feelings using words like 'Absolutely loved it' and 'Best'.\",\n",
            "      \"flavor\": \"Strawberry Cheesecake\",\n",
            "      \"rating\": 4,\n",
            "      \"sentiment\": \"POSITIVE\"\n",
            "    },\n",
            "    {\n",
            "      \"explanation\": \"Although the reviewer says 'Quite good', the phrase 'a bit too sweet for my taste' indicates a negative sentiment and the rating is 1.\",\n",
            "      \"flavor\": \"Mango Tango\",\n",
            "      \"rating\": 1,\n",
            "      \"sentiment\": \"NEGATIVE\"\n",
            "    }\n",
            "  ]\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"ARRAY\",\n",
        "        \"items\": {\n",
        "            \"type\": \"OBJECT\",\n",
        "            \"properties\": {\n",
        "                \"rating\": {\"type\": \"INTEGER\"},\n",
        "                \"flavor\": {\"type\": \"STRING\"},\n",
        "                \"sentiment\": {\n",
        "                    \"type\": \"STRING\",\n",
        "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
        "                },\n",
        "                \"explanation\": {\"type\": \"STRING\"},\n",
        "            },\n",
        "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "prompt = \"\"\"\n",
        "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
        "\n",
        "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
        "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=prompt,\n",
        "    config=GenerateContentConfig(\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=response_schema,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV1dR-QlTKRs"
      },
      "source": [
        "## Count tokens and compute tokens\n",
        "\n",
        "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
        "\n",
        "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syx-fwLkV1j-"
      },
      "source": [
        "### Count tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UhNElguLRRNK",
        "outputId": "5dc4dad6-3f53-4c10-e7d8-806af7697736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_tokens=9 cached_content_token_count=None\n"
          ]
        }
      ],
      "source": [
        "response = client.models.count_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the highest mountain in Africa?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS-AP7AHUQmV"
      },
      "source": [
        "### Compute tokens\n",
        "\n",
        "The `compute_tokens()` method runs a local tokenizer instead of making an API call. It also provides more detailed token information such as the `token_ids` and the `tokens` themselves\n",
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>NOTE: This method is only supported in Vertex AI.</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Cdhi5AX1TuH0",
        "outputId": "94171623-92ab-4892-8505-ab8bdc5dc0ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens_info=[TokensInfo(role='user', token_ids=['1841', '235303', '235256', '573', '32514', '2204', '575', '573', '4645', '5255', '235336'], tokens=[b'What', b\"'\", b's', b' the', b' longest', b' word', b' in', b' the', b' English', b' language', b'?'])]\n"
          ]
        }
      ],
      "source": [
        "response = client.models.compute_tokens(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What's the longest word in the English language?\",\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BsP0vXOY7hg"
      },
      "source": [
        "## Search as a tool (Grounding)\n",
        "\n",
        "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
        "\n",
        "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
        "\n",
        "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_M_4RRBdO_3"
      },
      "source": [
        "### Google Search\n",
        "\n",
        "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
        "\n",
        "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeR09J3AZT4U"
      },
      "outputs": [],
      "source": [
        "google_search_tool = Tool(google_search=GoogleSearch())\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"When is the next total solar eclipse in the United States?\",\n",
        "    config=GenerateContentConfig(tools=[google_search_tool]),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "print(response.candidates[0].grounding_metadata)\n",
        "\n",
        "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYKAzG1sH-K1"
      },
      "source": [
        "### Vertex AI Search\n",
        "\n",
        "You can use a [Vertex AI Search data store](https://cloud.google.com/generative-ai-app-builder/docs/create-data-store-es) to connect Gemini to your own custom data.\n",
        "\n",
        "Follow the [get started guide for Vertex AI Search](https://cloud.google.com/generative-ai-app-builder/docs/try-enterprise-search) to create a data store and app, then add the data store ID in the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYDf4618IG5u"
      },
      "outputs": [],
      "source": [
        "data_store_id = \"YOUR_DATA_STORE_ID\"  # @param {type: \"string\"}\n",
        "\n",
        "vertex_ai_search_tool = Tool(\n",
        "    retrieval=Retrieval(\n",
        "        vertex_ai_search=VertexAISearch(\n",
        "            datastore=f\"projects/{PROJECT_ID}/locations/us/collections/default_collection/dataStores/{data_store_id}\"\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the company culture like?\",\n",
        "    config=GenerateContentConfig(tools=[vertex_ai_search_tool]),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))\n",
        "\n",
        "print(response.candidates[0].grounding_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0pb-Kh1xEHU"
      },
      "source": [
        "## Function calling\n",
        "\n",
        "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
        "\n",
        "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
        "\n",
        "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSUWWlrrlR-D"
      },
      "source": [
        "### Python Function (Automatic Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRR8HZhLlR-E"
      },
      "outputs": [],
      "source": [
        "def get_current_weather(location: str) -> str:\n",
        "    \"\"\"Example method. Returns the current weather.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    return random.choice([\"sunny\", \"raining\", \"snowing\", \"fog\"])\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"What is the weather like in Boston?\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[get_current_weather],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4syyLEClGcn"
      },
      "source": [
        "### OpenAPI Specification (Manual Function Calling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BDQPwgcxRN3"
      },
      "outputs": [],
      "source": [
        "get_destination = FunctionDeclaration(\n",
        "    name=\"get_destination\",\n",
        "    description=\"Get the destination that the user wants to go to\",\n",
        "    parameters={\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"destination\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Destination that the user wants to go to\",\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "destination_tool = Tool(\n",
        "    function_declarations=[get_destination],\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"I'd like to travel to Paris.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[destination_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(response.candidates[0].content.parts[0].function_call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhDs2X3o0neK"
      },
      "source": [
        "## Code Execution\n",
        "\n",
        "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The Gemini API provides code execution as a tool, similar to function calling.\n",
        "After you add code execution as a tool, the model decides when to use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W-3c7sy0nyz"
      },
      "outputs": [],
      "source": [
        "code_execution_tool = Tool(code_execution={})\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID,\n",
        "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
        "    config=GenerateContentConfig(\n",
        "        tools=[code_execution_tool],\n",
        "        temperature=0,\n",
        "    ),\n",
        ")\n",
        "for part in response.candidates[0].content.parts:\n",
        "    if part.executable_code:\n",
        "        print(\"Language:\", part.executable_code.language)\n",
        "        display(\n",
        "            Markdown(\n",
        "                f\"\"\"\n",
        "```\n",
        "{part.executable_code.code}\n",
        "```\n",
        "\"\"\"\n",
        "            )\n",
        "        )\n",
        "    if part.code_execution_result:\n",
        "        print(\"\\nOutcome:\", part.code_execution_result.outcome)\n",
        "        display(Markdown(f\"`{part.code_execution_result.output}`\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQwiONFdVHw5"
      },
      "source": [
        "## What's next\n",
        "\n",
        "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
        "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
        "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hIJVEr0RQY8S",
        "rZV2TY5Pa3Dd",
        "hYKAzG1sH-K1",
        "mSUWWlrrlR-D",
        "h4syyLEClGcn"
      ],
      "name": "intro_gemini_2_0_flash.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}